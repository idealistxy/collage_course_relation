# 步骤1：数据准备
import pandas as pd

# 加载数据集
column_names = ['age', 'workclass', 'fnlwgt', 'education',
                'education-num', 'marital-status', 'occupation', 'relationship',
                'race', 'sex', 'capital-gain', 'capital-loss',
                'hours-per-week', 'native-country', 'income']
train_data = pd.read_csv('D:/大三课程/机器学习/实验/实验七/adult_train.txt',
                         names=column_names, na_values=' ?', delimiter=',')
test_data = pd.read_csv('D:/大三课程/机器学习/实验/实验七/adult_test.txt',
                        names=column_names, na_values=' ?', delimiter=',')
# 填充训练集和测试集中的缺失值为众数
train_data.fillna(train_data.mode().iloc[0], inplace=True)
test_data.fillna(test_data.mode().iloc[0], inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns
# 合并训练集和测试集
combined_data = pd.concat([train_data, test_data], ignore_index=True)

# 提取数值特征
numeric_features = combined_data.select_dtypes(include=['int64', 'float64'])
# 设置图形大小
plt.figure(figsize=(15, 10))

# 循环遍历每个数值特征，分别绘制箱线图
for i, col in enumerate(numeric_features.columns):
    plt.subplot(2, 3, i + 1)  
    sns.boxplot(y=combined_data[col])
    plt.title(col)
    plt.xticks(rotation=45)  

plt.tight_layout() 
plt.show()
sns.set(style="whitegrid")
plt.figure(figsize=(10, 8))
for i, col in enumerate(numeric_features.columns):
    plt.subplot(3, 2, i + 1)
    sns.histplot(combined_data[col], bins=20, kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()
train_y = train_data['income'].map({' <=50K': 0, ' >50K': 1})
test_y = test_data['income'].map({' <=50K.': 0, ' >50K.': 1})
y = pd.concat([train_y, test_y], ignore_index=True)

# 合并特征和标签为一个DataFrame
data_with_label = pd.concat([numeric_features, y], axis=1)
# # 绘制特征与标签的散点图
# sns.pairplot(data_with_label, hue='income')
# plt.show()
# 计算特征与标签的相关性矩阵
correlation_matrix = data_with_label.corr()

# 绘制相关性矩阵的热力图
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap between Features and Income')
plt.show()
# 步骤2：数据预处理

# 删除缺失值和'native-country'属性
train_data.dropna(inplace=True)
train_data.drop(columns=['native-country'], inplace=True)
# train_data.drop(columns=['fnlwgt'], inplace=True)
# train_data.drop(columns=['capital-gain'], inplace=True)
# train_data.drop(columns=['capital-loss'], inplace=True)
# train_data.drop(columns=['hours-per-week'], inplace=True)

test_data.dropna(inplace=True)
# test_data.drop(columns=['fnlwgt'], inplace=True)
test_data.drop(columns=['native-country'], inplace=True)
# test_data.drop(columns=['capital-gain'], inplace=True)
# test_data.drop(columns=['capital-loss'], inplace=True)
# test_data.drop(columns=['hours-per-week'], inplace=True)
# 步骤3：特征工程
# 特征选择、特征变换等

# # 将标签编码为0和1
train_data['income'] = train_data['income'].map({' <=50K': 0, ' >50K': 1})
test_data['income'] = test_data['income'].map({' <=50K.': 0, ' >50K.': 1})

# 合并训练集和测试集
combined_data = pd.concat([train_data, test_data], ignore_index=True)
# 独热编码
combined_data = pd.get_dummies(combined_data, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])

# 提取特征和标签
y_combined = combined_data['income']
X_combined = combined_data.drop(columns=['income'])


# 将分类变量映射为整数编码
for column in X_combined.select_dtypes(include=['object']).columns:
    X_combined[column] = X_combined[column].astype('category').cat.codes

# 划分回训练集和测试集
X_train = X_combined.iloc[:len(train_data), :]
X_test = X_combined.iloc[len(train_data):, :]

y_train = train_data['income']
y_test = test_data['income']

feature_names = X_combined.columns
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
# 步骤4：模型训练
# 逻辑回归模型
lr_model = LogisticRegression(
    penalty = "l2",
    dual = False,
    tol = 1e-6,
    C = 1,
    fit_intercept = True,
    intercept_scaling = 1,
    class_weight = "balanced",
    random_state = 42,
    solver = "liblinear",
    max_iter = 100,
    multi_class = "auto",
    verbose = 0,
    warm_start = False,
    n_jobs = True,
    l1_ratio = None
)

lr_model.fit(X_train, y_train)
lr_preds = lr_model.predict(X_test)

# 计算指标
lr_acc = accuracy_score(y_test, lr_preds)
lr_auc = roc_auc_score(y_test, lr_preds)


# 输出结果
print("逻辑回归模型准确率：", lr_acc)
print("逻辑回归模型AUC:", lr_auc)
# from sklearn.model_selection import GridSearchCV

# # 定义超参数的网格
# param_grid = {
#     'n_estimators': [30, 50, 100, 150, 200],
#     'max_depth': [10, 15, 20, 25, 30],
#     'min_samples_split': [10, 20, 30, 40, 50],
#     'min_samples_leaf': [5, 10, 15, 20, 25]
# }

# # 创建随机森林分类器
# rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)

# # 创建网格搜索对象
# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)

# # 在训练数据上执行网格搜索
# grid_search.fit(X_train, y_train)

# # 输出最佳参数
# print("Best parameters found: ", grid_search.best_params_)

# 随机森林模型
rf_model = RandomForestClassifier(
    n_estimators = 100,
    criterion= "gini",
    max_depth =30,
    min_samples_split= 30,
    min_samples_leaf = 5,
    min_weight_fraction_leaf = 0,
    max_features = "sqrt",
    max_leaf_nodes = None,
    min_impurity_decrease = 0,
    bootstrap = True,
    oob_score = True,
    n_jobs = None,
    random_state = None,
    verbose = 0,
    warm_start = False,
    class_weight = 'balanced',
    ccp_alpha = 0,
    max_samples = None
)


rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# 计算指标
rf_acc = accuracy_score(y_test, rf_preds)
rf_auc = roc_auc_score(y_test, rf_preds)
print("随机森林模型准确率：", rf_acc)
print("随机森林模型AUC:", rf_auc)
# 获取特征系数
coefficients = lr_model.coef_[0]

# 创建一个包含特征名和对应系数的字典
feature_coef_dict = dict(zip(feature_names, abs(coefficients)))

# 对特征系数进行排序
sorted_feature_coef = sorted(feature_coef_dict.items(), key=lambda x: x[1], reverse=True)

# 选择前15个重要的特征
top_15_features = dict(sorted_feature_coef[:15])

# 提取前15个特征的名称和系数
top_15_feature_names = list(top_15_features.keys())
top_15_feature_coef = list(top_15_features.values())

# 绘制水平条形图
plt.barh(range(len(top_15_feature_names)), top_15_feature_coef, tick_label=top_15_feature_names)
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 15 Important Features (Logistic Regression)')

plt.show()
feat_importance = rf_model.feature_importances_
feature_names = X_combined.columns

# 创建一个包含特征名和对应重要性的字典
feature_importance_dict = dict(zip(feature_names, feat_importance))

# 对特征重要性进行排序
sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# 选择前10个重要的特征
top_15_features = dict(sorted_feature_importance[:15])

# 提取前10个特征的名称和重要性
top_15_feature_names = list(top_15_features.keys())
top_15_feature_importance = list(top_15_features.values())

# 绘制水平条形图
plt.barh(range(len(top_15_feature_names)), top_15_feature_importance, tick_label=top_15_feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Top 15 Important Features (RandomForestClassifier)')

plt.show()
